{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://www.kaggle.com/kyakovlev/ieee-data-minification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, warnings, random\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Helpers\n",
    "#################################################################################\n",
    "## -------------------\n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "## ------------------- \n",
    "\n",
    "## -------------------\n",
    "## Memory Reducer\n",
    "# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n",
    "# :verbose                                        # type: bool\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "## -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Vars\n",
    "#################################################################################\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "LOCAL_TEST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Data\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "########################### DATA LOAD \n",
    "print('Load Data')\n",
    "folder_path = '../Inputs/'\n",
    "train_identity = pd.read_csv(f'{folder_path}train_identity.csv')\n",
    "train_df = pd.read_csv(f'{folder_path}train_transaction.csv')\n",
    "\n",
    "test_identity = pd.read_csv(f'{folder_path}test_identity.csv')\n",
    "test_df = pd.read_csv(f'{folder_path}test_transaction.csv')\n",
    "test_df['isFraud'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 542.35 Mb (69.4% reduction)\n",
      "Mem. usage decreased to 473.07 Mb (68.9% reduction)\n",
      "Mem. usage decreased to 25.86 Mb (42.7% reduction)\n",
      "Mem. usage decreased to 25.44 Mb (42.7% reduction)\n",
      "Wall time: 5min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "########################### Base Minification\n",
    "train_df = reduce_mem_usage(train_df)\n",
    "test_df  = reduce_mem_usage(test_df)\n",
    "\n",
    "train_identity = reduce_mem_usage(train_identity)\n",
    "test_identity  = reduce_mem_usage(test_identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7f9d6184cea0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'card4'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train['card4'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['card6'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### card4, card6, ProductCD\n",
    "# Converting Strings to ints(or floats if nan in column) using frequency encoding\n",
    "# We will be able to use these columns as category or as numerical feature\n",
    "\n",
    "for col in ['card4', 'card6', 'ProductCD']:\n",
    "    print('Encoding', col)\n",
    "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    col_encoded = temp_df[col].value_counts().to_dict()   \n",
    "    train_df[col] = train_df[col].map(col_encoded)\n",
    "    test_df[col]  = test_df[col].map(col_encoded)\n",
    "    print(col_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['card4'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['card6'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### M columns\n",
    "#################################################################################\n",
    "# Converting Strings to ints(or floats if nan in column)\n",
    "\n",
    "for col in ['M1','M2','M3','M5','M6','M7','M8','M9']:\n",
    "    train_df[col] = train_df[col].map({'T':1, 'F':0})\n",
    "    test_df[col]  = test_df[col].map({'T':1, 'F':0})\n",
    "\n",
    "for col in ['M4']:\n",
    "    print('Encoding', col)\n",
    "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    col_encoded = temp_df[col].value_counts().to_dict()   \n",
    "    train_df[col] = train_df[col].map(col_encoded)\n",
    "    test_df[col]  = test_df[col].map(col_encoded)\n",
    "    print(col_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Identity columns\n",
    "\n",
    "def minify_identity_df(df):\n",
    "\n",
    "    df['id_12'] = df['id_12'].map({'Found':1, 'NotFound':0})\n",
    "    df['id_15'] = df['id_15'].map({'New':2, 'Found':1, 'Unknown':0})\n",
    "    df['id_16'] = df['id_16'].map({'Found':1, 'NotFound':0})\n",
    "\n",
    "    df['id_23'] = df['id_23'].map({'TRANSPARENT':4, 'IP_PROXY':3, 'IP_PROXY:ANONYMOUS':2, 'IP_PROXY:HIDDEN':1})\n",
    "\n",
    "    df['id_27'] = df['id_27'].map({'Found':1, 'NotFound':0})\n",
    "    df['id_28'] = df['id_28'].map({'New':2, 'Found':1})\n",
    "\n",
    "    df['id_29'] = df['id_29'].map({'Found':1, 'NotFound':0})\n",
    "\n",
    "    df['id_35'] = df['id_35'].map({'T':1, 'F':0})\n",
    "    df['id_36'] = df['id_36'].map({'T':1, 'F':0})\n",
    "    df['id_37'] = df['id_37'].map({'T':1, 'F':0})\n",
    "    df['id_38'] = df['id_38'].map({'T':1, 'F':0})\n",
    "\n",
    "    df['id_34'] = df['id_34'].fillna(':0')\n",
    "    df['id_34'] = df['id_34'].apply(lambda x: x.split(':')[1]).astype(np.int8)\n",
    "    df['id_34'] = np.where(df['id_34']==0, np.nan, df['id_34'])\n",
    "    \n",
    "    df['id_33'] = df['id_33'].fillna('0x0')\n",
    "    df['id_33_0'] = df['id_33'].apply(lambda x: x.split('x')[0]).astype(int)\n",
    "    df['id_33_1'] = df['id_33'].apply(lambda x: x.split('x')[1]).astype(int)\n",
    "    df['id_33'] = np.where(df['id_33']=='0x0', np.nan, df['id_33'])\n",
    "\n",
    "    df['DeviceType'].map({'desktop':1, 'mobile':0})\n",
    "    return df\n",
    "\n",
    "train_identity = minify_identity_df(train_identity)\n",
    "test_identity = minify_identity_df(test_identity)\n",
    "\n",
    "for col in ['id_33']:\n",
    "    train_identity[col] = train_identity[col].fillna('unseen_before_label')\n",
    "    test_identity[col]  = test_identity[col].fillna('unseen_before_label')\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(train_identity[col])+list(test_identity[col]))\n",
    "    train_identity[col] = le.transform(train_identity[col])\n",
    "    test_identity[col]  = le.transform(test_identity[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Final Minification\n",
    "\n",
    "train_df = reduce_mem_usage(train_df)\n",
    "test_df  = reduce_mem_usage(test_df)\n",
    "\n",
    "train_identity = reduce_mem_usage(train_identity)\n",
    "test_identity  = reduce_mem_usage(test_identity)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Export\n",
    "\n",
    "train_df.to_pickle('train_transaction.pkl')\n",
    "test_df.to_pickle('test_transaction.pkl')\n",
    "\n",
    "train_identity.to_pickle('train_identity.pkl')\n",
    "test_identity.to_pickle('test_identity.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
